{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e609700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "686c3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    # Set default device for new tensors\n",
    "    torch.set_default_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ec363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    data: str | pd.DataFrame,\n",
    "    drop_columns: list = None,\n",
    "    ordinal_encode_columns: list = None,\n",
    "    one_hot_encode_columns: list = None,\n",
    "    drop_na: bool = True,\n",
    "):\n",
    "    if isinstance(data, str):\n",
    "        df = pd.read_csv(data)\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        df = data\n",
    "    else:\n",
    "        raise ValueError(\"Input data must be a file path (str) or a pandas DataFrame.\")\n",
    "    ordinal_mappings = {}\n",
    "\n",
    "    if drop_columns:\n",
    "        df = df.drop(columns=drop_columns)\n",
    "    if ordinal_encode_columns:\n",
    "        ordinal_encoder = OrdinalEncoder()\n",
    "        df[ordinal_encode_columns] = ordinal_encoder.fit_transform(df[ordinal_encode_columns])\n",
    "        for i, col in enumerate(ordinal_encode_columns):\n",
    "            ordinal_mappings[col] = {category: index for index, category in enumerate(ordinal_encoder.categories_[i])}\n",
    "    if one_hot_encode_columns:\n",
    "        one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "        one_hot_encoded = one_hot_encoder.fit_transform(df[one_hot_encode_columns])\n",
    "        one_hot_encoded_df = pd.DataFrame(\n",
    "            one_hot_encoded, columns=one_hot_encoder.get_feature_names_out(one_hot_encode_columns)\n",
    "        )\n",
    "        df = pd.concat([df.drop(columns=one_hot_encode_columns), one_hot_encoded_df], axis=1)\n",
    "\n",
    "    if drop_na:\n",
    "        df = df.dropna()\n",
    "\n",
    "    return df, ordinal_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCClass(Dataset):\n",
    "    def __init__(self, df, scaler=None, scale_method_cls=StandardScaler):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        data = self.df.astype(np.float32)\n",
    "        if scaler is None:\n",
    "            # Usually for training, we want to fit the scaler\n",
    "            self.scaler = scale_method_cls()\n",
    "            scaled_data = self.scaler.fit_transform(data)\n",
    "        else:\n",
    "            # For validation/test\n",
    "            self.scaler = scaler\n",
    "            scaled_data = self.scaler.transform(data)\n",
    "        self.data_tensor = torch.tensor(scaled_data, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_tensor[idx]\n",
    "\n",
    "    def get_scaler(self):\n",
    "        return self.scaler\n",
    "\n",
    "\n",
    "class SCDataLoader(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: str,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 0,\n",
    "        val_split: float = 0.15,\n",
    "        test_split: float = 0.15,\n",
    "        drop_columns: list = None,\n",
    "        ordinal_encode_columns: list = None,\n",
    "        one_hot_encode_columns: list = None,\n",
    "        drop_na: bool = True,\n",
    "        scale_method_cls=StandardScaler,\n",
    "        random_seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.data: pd.DataFrame = None\n",
    "        self.processed_data: pd.DataFrame = None\n",
    "        self.ordinal_mappings: dict = None\n",
    "        self.train_dataset: SCClass = None\n",
    "        self.val_dataset: SCClass = None\n",
    "        self.test_dataset: SCClass = None\n",
    "        self.scaler: StandardScaler = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if self.train_dataset is not None and self.val_dataset is not None:\n",
    "            # Check for test dataset existence only if test split is expected\n",
    "            if self.test_dataset is not None:\n",
    "                return\n",
    "        data = self.hparams.data\n",
    "        if isinstance(data, str):\n",
    "            self.data = pd.read_csv(data)\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            self.data = data\n",
    "        else:\n",
    "            raise ValueError(\"Input data must be a file path (str) or a pandas DataFrame.\")\n",
    "\n",
    "        if self.hparams.test_split == 0:\n",
    "            raise ValueError(\"test_split must be greater than 0.\")\n",
    "        self.processed_data, self.ordinal_mappings = preprocess_data(\n",
    "            self.data,\n",
    "            drop_columns=self.hparams.drop_columns,\n",
    "            ordinal_encode_columns=self.hparams.ordinal_encode_columns,\n",
    "            one_hot_encode_columns=self.hparams.one_hot_encode_columns,\n",
    "            drop_na=self.hparams.drop_na,\n",
    "        )\n",
    "\n",
    "        # Splitting and dataset initiations\n",
    "        train_df, test_df = train_test_split(\n",
    "            self.processed_data, test_size=self.hparams.test_split, random_state=self.hparams.random_seed\n",
    "        )\n",
    "        train_df, val_df = train_test_split(\n",
    "            train_df,\n",
    "            test_size=self.hparams.val_split / (1 - self.hparams.test_split),\n",
    "            random_state=self.hparams.random_seed,\n",
    "        )\n",
    "        self.train_dataset = SCClass(train_df, scale_method_cls=self.hparams.scale_method_cls)\n",
    "        self.scaler = self.train_dataset.get_scaler()\n",
    "        self.val_dataset = SCClass(val_df, scaler=self.scaler)\n",
    "        self.test_dataset = SCClass(test_df, scaler=self.scaler)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            persistent_workers=True if self.hparams.num_workers > 0 else False,\n",
    "            pin_memory=True,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            persistent_workers=True if self.hparams.num_workers > 0 else False,\n",
    "            pin_memory=True,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            persistent_workers=True if self.hparams.num_workers > 0 else False,\n",
    "            pin_memory=True,\n",
    "            shuffle=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f80dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78362b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0ec21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b799fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata = ad.read_h5ad('/Users/lukashat/Documents/PhD_Schapiro/Projects/Myeloma_Standal/results/standard/adatas/cells_final.h5ad')\n",
    "# adata.layers['raw'] = adata.raw.X\n",
    "# adata.X = adata.layers['arcsinh']\n",
    "# adata.X = adata.X.astype('float32')\n",
    "# adata.obs = adata.obs.drop(columns=['Phenotype4','disease2', 'disease3', 'image_ID', 'disease', 'cellcharter_CN', 'HistoneH3', 'ROI'])\n",
    "# dataloader = AnnLoader(adata,\n",
    "#    batch_size=32,\n",
    "#    shuffle=True,\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20e587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scpvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
